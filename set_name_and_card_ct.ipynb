{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set and Card Counts\n",
    "This script scrapes bulbapedia to generate a list of pokemon card sets and their respective card counts. It currently scrapes only the English sets, as the Japanese cards don't have numbers or set numbers. This script should be run once daily to check for new sets, and add them to the db."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "sets_url = 'https://bulbapedia.bulbagarden.net/wiki/List_of_Pok%C3%A9mon_Trading_Card_Game_expansions'\n",
    "r_sets = requests.get(sets_url, verify=False)\n",
    "soup_sets = BeautifulSoup(r_sets.text, \"html.parser\")\n",
    "\n",
    "bulbapedia_base_url = 'https://bulbapedia.bulbagarden.net'  # base bulbapedia url\n",
    "\n",
    "# list of all sets in <a href=\"...\" title=\"<set name> (TCG)\">set name</a> form\n",
    "set_link_list = soup_sets.select('tbody > tr > td > a[href*=\"_(TCG)\"]')\n",
    "href_list_full = [el['href'] for el in set_link_list]  # get '/wiki/<set>_(TCG)' text from url\n",
    "# generate url from href and base url\n",
    "url_list_full = [bulbapedia_base_url + el for el in href_list_full]\n",
    "\n",
    "# break the url list off at \"Wizards Black Star Promos\"\n",
    "wizards_re_url = re.compile('.*/wiki/Wizards.*')  # not flexible if a set with \"Wizards\" in name drops\n",
    "wizards_promo_href = [string for string in url_list_full if re.match(wizards_re_url, string)]\n",
    "wizards_promo_href_ix = url_list_full.index(wizards_promo_href[0])\n",
    "url_list = url_list_full[:wizards_promo_href_ix]  # grab everything in url_list_full up to Wizards promo\n",
    "\n",
    "# create a list of set names\n",
    "set_name_list = [el.split('/')[-1].replace('_', ' ').replace('%26', '&')[:-6] for el in url_list]\n",
    "\n",
    "# generate list of requests from url_list\n",
    "r_list = [requests.get(url, verify=False) for url in url_list]\n",
    "# generate soup list from r_list\n",
    "r_soup_list = [BeautifulSoup(r.text, 'html.parser') for r in r_list]\n",
    "\n",
    "'''\n",
    "find the table at the link for each set with #/### and card name. There a few different cases here, \n",
    "so we start with a tmp table, then filter it down to make the real table. The different cases are:\n",
    "\n",
    "1) Set with English and Japanese versions\n",
    "Two or more tables with card no. and card name under \"Set list\" header on bulbapedia. The key \n",
    "here is to look for elements with style=\"text-align:left\"\n",
    "ex. Base Set\n",
    "\n",
    "2) Set with NO Japanese version\n",
    "One table with card no. and card name under \"Set list\" header on bulbapedia. The key here is to\n",
    "look for elements with style=\"float:left\". There are also instances where more than one column exists,\n",
    "like with the \"Diamond & Pearl\" set. In this case, the 'style=\"float:left\"' logic fails. Instead,\n",
    "we want the 'style=\"text-align:left\"' logic. This is because the former logic only picks up the\n",
    "header(s) of the table(s)\n",
    "ex. Base Set 2\n",
    "\n",
    "3) Set with NO English version\n",
    "We want to remove these completely. These are captured by the \"len(el) == 1\" logic, since the scraper\n",
    "only pulls the set name from the table header by selecting \"style='text-align:left'\"\n",
    "ex. Pokémon Card★VS\n",
    "\n",
    "4) Set with English and Japanese versions, where Japanese version has a link\n",
    "This is a set that will get picked up by our scraper, so we want to make sure to remove it. This is\n",
    "done later in the script when we clean up the elements returned in our soup\n",
    "ex. Moonlit Pursuit/Dawn Dash\n",
    "'''\n",
    "\n",
    "card_tbl_list_tmp = [el.select(\"[style*='text-align:left']\") for el in r_soup_list]\n",
    "\n",
    "'''\n",
    "list of char counts for elements grabbed by the select from r_soup_list above. It looks something\n",
    "like this: [[10000, 80], [70, 85], [81], [20000, 70, 25000, 80, 82], ...]. We will remove the\n",
    "[#] elements, as they're solely Japanese sets. We will change the \"select\" statement for the\n",
    "[<200, <200, ...] elements, since they picked up the table headers, not the actual content (case 2).\n",
    "Otherwise, we keep the elements, as they contain the data we're after\n",
    "'''\n",
    "\n",
    "char_ct_list = []\n",
    "for card_tbl in card_tbl_list_tmp:\n",
    "    tmp_list = []\n",
    "    for el in card_tbl:\n",
    "        tmp_list.append(len(str(el)))\n",
    "    char_ct_list.append(tmp_list)\n",
    "\n",
    "# list elements of the form [(<set name>, request element(s), [#, #, ...]), (\"), ...]\n",
    "name_tbl_ct_tuple = list(map(lambda x,y,z:(x,y,z), set_name_list, card_tbl_list_tmp, char_ct_list))\n",
    "    \n",
    "# create the final card table list for each set\n",
    "name_list = []\n",
    "tbl_list = []\n",
    "for ix,el in enumerate(name_tbl_ct_tuple):\n",
    "    if len(el[2]) == 1:  # remove \"Pokémon Card★VS\" and \"Pokémon Card★web\" Japanese sets\n",
    "        pass\n",
    "    # handle sets without Japanese versions, as well as instances where the scraper fails to\n",
    "    # pick up the content of the \"Set list\" table and instead grabs the table header(s)\n",
    "    elif all([num < 200 for num in el[2]]):\n",
    "        tbl_content_tmp = r_soup_list[ix].select(\"[style*='float:left']\")\n",
    "        name_list.append(el[0])\n",
    "        tbl_list.append(tbl_content_tmp)\n",
    "    else:  # keep what's already in card_tbl_list_tmp\n",
    "        name_list.append(el[0])\n",
    "        tbl_list.append(el[1])      \n",
    "                \n",
    "# turn the soup into text for easy parsing\n",
    "raw_txt_list = []\n",
    "for col in tbl_list:\n",
    "    tmp_list = []\n",
    "    for el in col:\n",
    "        tmp_list.append(el.text)\n",
    "    raw_txt_list.append(tmp_list)\n",
    "\n",
    "# grab all the elements with \"/\" in them - we are looking for #/### to get the # of cards in the set\n",
    "sets_txt_list = []\n",
    "for group in raw_txt_list:\n",
    "    tmp_list = []\n",
    "    for txt in group:\n",
    "        if '/' in txt: tmp_list.append(txt)\n",
    "    sets_txt_list.append(tmp_list)\n",
    "    \n",
    "# remove '\\n' and whitespace from sets_txt_list\n",
    "sets_txt_clean_list = []\n",
    "name_remove_ixs = []\n",
    "for ix,list_ in enumerate(sets_txt_list):\n",
    "    tmp_list = []\n",
    "    for el in list_:\n",
    "        tmp_list.append(el.replace('\\n', ' '))\n",
    "    if len(tmp_list) > 0:  # removes Moonlit Pursuit/Dawn Dash set(s)\n",
    "        sets_txt_clean_list.append(tmp_list)\n",
    "    else:\n",
    "        name_remove_ixs.append(ix)\n",
    "        \n",
    "# remove the set(s) from name_list that don't have values in their cleaned text list\n",
    "for ix,ix_to_pop in enumerate(name_remove_ixs):\n",
    "    # if there is more than one element to pop, this makes sure we pop the right one as name_list \n",
    "    # gets shorter\n",
    "    name_list.pop(ix_to_pop - ix)\n",
    "\n",
    "# create list of strings in sets_txt_clean_list for ease of parsing to search for ###/###\n",
    "sets_txt_clean_strs_list = [el[0].split() for el in sets_txt_clean_list]\n",
    "\n",
    "set_ct_re = re.compile('\\d+\\/\\d+')  # regex to match ###/### format\n",
    "set_ct_list = []\n",
    "# pull\n",
    "for el in sets_txt_clean_strs_list:\n",
    "    tmp_list = []\n",
    "    for string in el:\n",
    "        if re.match(set_ct_re, string):\n",
    "            tmp_list.append(string)\n",
    "    set_ct_list.append(int(tmp_list[0].split('/')[1]))  # pull the ### right of the slash\n",
    "\n",
    "# final list that has the set name along with how many cards are in it!!\n",
    "name_ct_list = list(map(lambda x,y:(x,y), name_list, set_ct_list))\n",
    "\n",
    "# create a pd df\n",
    "set_df = pd.DataFrame({'Set Name': name_list, 'Card Count': set_ct_list})\n",
    "\n",
    "# save it to an excel\n",
    "set_df.to_excel('Set_and_Card_Ct.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
